# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uM5ZIVzJq_FAanP1h_s0U-hUR1Galbnw
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.model_selection import train_test_split
from keras import models
from keras.layers import Activation
from keras.utils import np_utils
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers import Flatten
from keras.datasets import mnist
from PIL import Image
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator

#Reshape the data into(300,300,1) and applying one_hot encoding
def reshape(data,y_1):
  x = data.reshape((data.shape[0], data.shape[1], data.shape[2], 1)).astype('float32')
  onehot = np_utils.to_categorical(y_1)
  return x,onehot

#function to load the classroom Data 
def created_data(data_path,label_path):
  #loading the data
  x_data = np.load(data_path)
  y_data = np.load(label_path)
  
  #converting the data into binary form
  denoise_data = []
  threshold = 120/255
  for i in range(x_data.shape[0]):
    denoise_data.append(1.0 * (x_data[i] > threshold))
  denoise_data = np.array(denoise_data)

  #Inverting the data
  Invert_data = []
  threshold = 150/255
  for i in range(denoise_data.shape[0]):
    Invert_data.append(1.0 * (denoise_data[i] < threshold))
  Invert_data=np.array(Invert_data)

  return reshape(Invert_data,y_data)

#function to load MNIST data
#rescale the data to (300,300)
def re_scaledata(data):
  rescaled_MNIST_data = []
  for i in range(data.shape[0]):
    img1 = Image.fromarray(data[i])
    img2 = img1.crop( (-35,-35,65,65) )
    newsize = (300,300)
    img3 = img2.resize(newsize)
    rescaled_MNIST_data.append(np.asarray(img3))
  return np.array(rescaled_MNIST_data)  

def mnist_data(size):
  #loadin the mnist data and normalizing
  (X_train_mnist, Y_train_mnist), (X_test_mnist, Y_test_mnist) = mnist.load_data()
  X_train_mnist = X_train_mnist/255.0
  X_test_mnist = X_test_mnist/255.0
  x_train_mnist=np.array(X_train_mnist)
  y_train_mnist=np.array(Y_train_mnist)
  x_test_mnist=np.array(X_test_mnist)
  y_test_mnist=np.array(Y_test_mnist)

  #Taking only a part of the data
  x_strain_mnist = re_scaledata(x_train_mnist[:size,:,:])
  x_stest_mnist= re_scaledata(x_test_mnist[:size,:,:])
  y_strain_mnist=y_train_mnist[:size]
  y_stest_mnist=y_test_mnist[:size]
  
  return reshape(x_strain_mnist,y_strain_mnist),reshape(x_stest_mnist,y_stest_mnist)

"""## Path for the data and labels"""

data_path =r'X_train.npy'
label_path =r'y_train.npy'
Invert_data,y_data=created_data(data_path,label_path)
x_train, created_data_test, y_train, created_label_test = train_test_split(Invert_data, y_data, test_size=0.1, random_state=21)
created_data_train,created_data_val,created_label_train,created_label_val=train_test_split(x_train,y_train,test_size=0.1,random_state=21)

"""## Parameter for size of the mnist data set"""

size_mnist=3500
(mnist_data_train, mnist_label_train), (mnist_data_test, mnist_label_test) = mnist_data(size_mnist)

"""## size of the test set"""

#combining the data training data
combined_data = np.concatenate([created_data_train,mnist_data_train],axis=0)
combined_label = np.concatenate([created_label_train,mnist_label_train]) 

#Train_test split
test_size=0.1
comb_data_train, comb_data_test, comb_label_train, comb_label_test = train_test_split(combined_data,combined_label, test_size=test_size, random_state=21)

def CNN_model(data):
  model = Sequential()
  model.add(Conv2D(4, (3, 3), activation='relu', input_shape=(data.shape[1], data.shape[2],1)))
  model.add(MaxPooling2D((2, 2)))
  model.add(Conv2D(16, (3, 3), activation='relu'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Conv2D(32, (3, 3), activation='relu'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Conv2D(64, (3, 3), activation='relu'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Conv2D(128, (3, 3), activation='relu'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Conv2D(512, (3, 3), activation='relu'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Flatten())
  model.add(Dense(256,activation='relu'))
  model.add(Dense(128,activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(10, activation='softmax'))
	#opt = SGD(lr=0.009, momentum=0.9)
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  return model

"""## Hyperparamters 
Batch_size
no. of epochs
"""

#training the model with parameters as below
batch_size=64
epochs=19

model = CNN_model(comb_data_train)
datagen = ImageDataGenerator(rescale=1.0/1.0)
train_iterator = datagen.flow(comb_data_train, comb_label_train, batch_size=batch_size)
model.fit(comb_data_train,comb_label_train, validation_data=(created_data_val,created_label_val), epochs=epochs)

"""## Evaluating the model
1. With the classroom data
2. With the Mnist data
3. With the combined data

Just to avoid overfitting trying to get a better accuracy and low loss for all the 3 test dataset
"""

#Evalutaing the data
created_loss,created_acc=model.evaluate(created_data_test,created_label_test)
print('Classroom data : ' + str(created_loss) +' , '+ str(created_acc))
mnist_loss,mnist_acc=model.evaluate(mnist_data_test,mnist_label_test)
print("MNIST data : " + str(mnist_loss)+' , '+str(mnist_acc))
comb_loss,comb_acc=model.evaluate(comb_data_test,comb_label_test)
print("Combined data : "+str(comb_loss) +' , '+str(comb_acc))

model.save('new_model')

print("Model has been saved to cureent directory as new_model")